{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/abdn-cs3033-ai/practicals/blob/main/week10/tutorial9-rl.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS3033: Artificial Intelligence\n",
    "\n",
    "## Tutorial 09: Reinforcement Learning\n",
    "\n",
    "#### Prof. Felipe Meneguzzi\n",
    "\n",
    "Adapted from code in the [AIMA-Python](https://github.com/aimacode/aima-python) public repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to run this tutorial, you need to download the auxiliary files from Github into your notebook, which we do with Jupyter's shell commands (if you downloaded the entire repo, the code below is not necessary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  import google.colab\n",
    "  print(\"We are in Google colab, we need to clone the repo\")\n",
    "  !pip3 install seaborn --user\n",
    "  !git clone https://github.com/abdn-cs3033-ai/practicals.git\n",
    "  %cd practicals/week10\n",
    "except:\n",
    "  print(\"Not in colab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Overview](#overview)\n",
    "- [Passive RL](#passive-reinforcement-learning)\n",
    "- [Active RL](#active-reinforcement-learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "**Reinforcement Learning** is a family of machine learning techniques concerned with learning how to act in an environment where the agent does not fully know its dynamics. In reinforcement learning, an agent learns how to act in an environment by trial and error, interacting with the environment over repeated trials or episodes in which the agent tries different actions and observes a reward signal. Unlike supervised learning, reinforcement learning provides no explicit *right answer*, but rather a potentially sparse reward signal, as the agent acts in the environment.\n",
    "\n",
    "### MDPs\n",
    "\n",
    "The key assumption behind most Reinforcement Learning algorithms is that the environment behaves like an MDP, even if the agent is not aware of all its parameters. Recall the elements of an MDP are the following:\n",
    "\n",
    "- A finite set of states $\\mathcal{S}$ (known by the agent)\n",
    "- A finite set of actions $\\mathcal{A}$ (known by the agent)\n",
    "- A *markovian* transition model $T(s,a,s') = \\mathbb{P}(S_{t+1}=s' \\mid S_t = s, A_t = a)$ (possibly known by the agent)\n",
    "- A reward function $R(s)$, alternatively $R(s,a) = \\mathbb{E}[ R_{t+1} \\mid S_t = s, A_t = a]$ (unknown by the agent)\n",
    "- A discount factor $\\gamma$ (known by the agent)\n",
    "\n",
    "We will use an implementation of an MDP (and its algorithms), as an oracle against which we can compare our reinforcement learning approaches. Recall our "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from utils4e import vector_add, orientations, turn_right, turn_left\n",
    "from notebook import psource, pseudocode\n",
    "from mdp4e import MDP, value_iteration, GridMDP\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passive Reinforcement Learning\n",
    "\n",
    "In passive Reinforcement Learning the agent follows a fixed policy $\\pi$. Passive learning attempts to evaluate the given policy $pi$ - without any knowledge of the Reward function $R(s)$ and the Transition model $P(s' \\mid s, a)$.\n",
    "\n",
    "This is usually done by some method of **utility estimation** or **prediction**. The agent attempts to directly learn the utility of each state that would result from following the policy. Note that at each step, it has to *perceive* the reward and the state - it has no global knowledge of these. Thus, if in a certain state the entire set of actions offers a very low probability of attaining some state $s_+$ - the agent may never perceive the reward $R(s_+)$.\n",
    "\n",
    "Consider a situation where an agent is given a policy to follow. Thus, at any point it knows only its current state and current reward, and the action it must take next. This action may lead it to more than one state, with different probabilities.\n",
    "\n",
    "For a series of actions given by $\\pi$, the estimated utility $U$:\n",
    "$$U^{\\pi}(s) = E\\left[\\sum_{t=0}^{\\infty} \\gamma^{t} R(s_{t})\\right]$$\n",
    "Or the expected value of summed discounted rewards until termination.\n",
    "\n",
    "In this tutorial, we implement the method of utility estimation called Temporal Difference (TD) Learning. Instead of explicitly building the transition model $P$, the temporal-difference model uses the expected closeness between the utilities of two consecutive states $s$ and $s'$.\n",
    " For the transition $s$ to $s'$, we update the utility of state $s'$ using the following formula:\n",
    "$$U^{\\pi}(s) \\gets U^{\\pi}(s) + \\alpha \\left( R(s) + \\gamma U^{\\pi}(s') - U^{\\pi}(s) \\right)$$\n",
    " This model implicitly incorporates the transition probabilities by being weighed for each state by the number of times it is achieved from the current state. Thus, over a number of iterations, it converges similarly to the Bellman equations.\n",
    " The advantage of the TD learning model is its relatively simple computation at each step, rather than having to keep track of various counts.\n",
    " For $n_s$ states and $n_a$ actions the ADP model would have $n_s \\times n_a$ numbers $N_{sa}$ and $n_s^2 \\times n_a$ numbers $N_{s' \\mid sa}$ to keep track of. The TD model must only keep track of a utility $U(s)$ for each state.\n",
    "\n",
    "Our first implementation will be of the Passive TD Agent we saw in the lecture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudocode(\"Passive-TD-Agent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, you will implement the passive TD algorithm above within the `__call__` method, which allows us to use this class as a function in the subsequent tests. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mdp4e import MDP, policy_evaluation\n",
    "\n",
    "# 21.2.3 Temporal-difference learning\n",
    "\n",
    "\n",
    "class PassiveTDAgent:\n",
    "    \"\"\"\n",
    "    [Figure 21.4]\n",
    "    The abstract class for a Passive (non-learning) agent that uses\n",
    "    temporal differences to learn utility estimates. Override update_state\n",
    "    method to convert percept to state and reward. The mdp being provided\n",
    "    should be an instance of a subclass of the MDP Class.\n",
    "\n",
    "    import sys\n",
    "    from mdp import sequential_decision_environment\n",
    "    north = (0, 1)\n",
    "    south = (0,-1)\n",
    "    west = (-1, 0)\n",
    "    east = (1, 0)\n",
    "    policy = {(0, 2): east, (1, 2): east, (2, 2): east, (3, 2): None, (0, 1): north, (2, 1): north,\n",
    "              (3, 1): None, (0, 0): north, (1, 0): west, (2, 0): west, (3, 0): west,}\n",
    "    agent = PassiveTDAgent(policy, sequential_decision_environment, alpha=lambda n: 60./(59+n))\n",
    "    for i in range(200):\n",
    "        run_single_trial(agent,sequential_decision_environment)\n",
    "\n",
    "    agent.U[(0, 0)] > 0.2\n",
    "    True\n",
    "    agent.U[(0, 1)] > 0.2\n",
    "    True\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, pi, mdp, alpha=None):\n",
    "\n",
    "        self.pi = pi\n",
    "        self.U = {s: 0. for s in mdp.states}\n",
    "        self.Ns = {s: 0 for s in mdp.states}\n",
    "        self.s = None\n",
    "        self.a = None\n",
    "        self.r = None\n",
    "        self.gamma = mdp.gamma\n",
    "        self.terminals = mdp.terminals\n",
    "\n",
    "        if alpha:\n",
    "            self.alpha = alpha\n",
    "        else:\n",
    "            self.alpha = lambda n: 1 / (1 + n)  # udacity video\n",
    "\n",
    "    def __call__(self, percept):\n",
    "        s1, r1 = self.update_state(percept)\n",
    "        pi, U, Ns, s, r = self.pi, self.U, self.Ns, self.s, self.r\n",
    "        alpha, gamma, terminals = self.alpha, self.gamma, self.terminals\n",
    "        #### YOUR CODE HERE ####\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        #########################\n",
    "        return self.a\n",
    "\n",
    "    def update_state(self, percept):\n",
    "        \"\"\"To be overridden in most cases. The default case\n",
    "        assumes the percept to be of type (state, reward).\"\"\"\n",
    "        return percept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstrating Passive TD-Learning\n",
    "\n",
    "To demonstrate these agents, we make use of the `GridMDP` object from the `MDP` module. `sequential_decision_environment` is similar to that used for the `MDP` notebook but has discounting with $\\gamma = 0.9$.\n",
    "\n",
    "Recall that we have a class for grids, such as the one we saw in the lecture, reproduced below.\n",
    "\n",
    "![Grid World](img/mdp-bare.svg \"Grid World MDP illustration\")\n",
    "\n",
    "We instantiate the object **`mdp`** of the class using a list of lists for both the transition and the sensor model. The code below instantiates the Grid World shown above.\n",
    "\n",
    "The `Agent-Program` can be obtained by creating an instance of the relevant `Agent-Class`. The `__call__` method allows the `Agent-Class` to be called as a function. The class needs to be instantiated with a policy ($\\pi$) and an `MDP` whose utility of states will be estimated.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mdp4e import sequential_decision_environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `sequential_decision_environment` is a GridMDP object as shown below. The rewards are **+1** and **-1** in the terminal states, and **-0.04** in the rest. \n",
    "\n",
    "![Grid World](img/rl-tutorial.svg \"Grid World MDP illustration\")\n",
    "\n",
    "Now we define actions and a policy similar to **Fig 21.1** in the book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action Directions\n",
    "north = (0, 1)\n",
    "south = (0,-1)\n",
    "west = (-1, 0)\n",
    "east = (1, 0)\n",
    "\n",
    "policy = {\n",
    "    (0, 2): east,  (1, 2): east,  (2, 2): east,   (3, 2): None,\n",
    "    (0, 1): north,                (2, 1): north,  (3, 1): None,\n",
    "    (0, 0): north, (1, 0): west,  (2, 0): west,   (3, 0): west, \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`PassiveTDAgent` uses temporal differences to learn utility estimates. We learn the difference between the states and backup the values to previous states.  Let us look into the source before we see some usage examples. In creating the `TDAgent`, we use the **same learning rate** $\\alpha$ as given in the footnote of the book for Figure 22.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TDagent = PassiveTDAgent(policy, sequential_decision_environment, alpha = lambda n: 60./(59+n))\n",
    "\n",
    "def run_single_trial(agent_program, mdp):\n",
    "    \"\"\"Execute trial for given agent_program\n",
    "    and mdp. mdp should be an instance of subclass\n",
    "    of mdp.MDP \"\"\"\n",
    "\n",
    "    def take_single_action(mdp, s, a):\n",
    "        \"\"\"\n",
    "        Select outcome of taking action a\n",
    "        in state s. Weighted Sampling.\n",
    "        \"\"\"\n",
    "        x = random.uniform(0, 1)\n",
    "        cumulative_probability = 0.0\n",
    "        for probability_state in mdp.T(s, a):\n",
    "            probability, state = probability_state\n",
    "            cumulative_probability += probability\n",
    "            if x < cumulative_probability:\n",
    "                break\n",
    "        return state\n",
    "\n",
    "    current_state = mdp.init\n",
    "    while True:\n",
    "        current_reward = mdp.R(current_state)\n",
    "        percept = (current_state, current_reward)\n",
    "        next_action = agent_program(percept)\n",
    "        if next_action is None:\n",
    "            break\n",
    "        current_state = take_single_action(mdp, current_state, next_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we run **200 trials** for the agent to estimate Utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(200)):\n",
    "    run_single_trial(TDagent,sequential_decision_environment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The calculated utilities are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n'.join([str(k)+':'+str(v) for k, v in TDagent.U.items()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison with value iteration\n",
    "\n",
    "We can also compare the utility estimates learned by our agent to those obtained via **value iteration**.\n",
    "\n",
    "**Note that value iteration has a priori knowledge of the transition table $P$, the rewards $R$, and all the states $s$.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U = value_iteration(sequential_decision_environment)\n",
    "print('\\n'.join([str(k)+':'+str(v) for k, v in U.items()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evolution of utility estimates over iterations\n",
    "\n",
    "We can explore how these estimates vary with time by using plots similar to **Fig 22.5a**. We will first enable `matplotlib` using the inline backend. We also define a function to collect the values of utilities at each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def graph_utility_estimates(agent_program, mdp, no_of_iterations, states_to_graph):\n",
    "    graphs = {state:[] for state in states_to_graph}\n",
    "    for iteration in range(1,no_of_iterations+1):\n",
    "        run_single_trial(agent_program, mdp)\n",
    "        for state in states_to_graph:\n",
    "            graphs[state].append((iteration, agent_program.U[state]))\n",
    "    for state, value in graphs.items():\n",
    "        state_x, state_y = zip(*value)\n",
    "        plt.plot(state_x, state_y, label=str(state))\n",
    "    plt.ylim([0,1.2])\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('U')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a plot of state $(2,2)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = PassiveTDAgent(policy, sequential_decision_environment, alpha=lambda n: 60./(59+n))\n",
    "graph_utility_estimates(agent, sequential_decision_environment, 500, [(2,2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to plot multiple states on the same plot. As expected, the utility of the finite state $(3,2)$ stays constant and is equal to $R((3,2)) = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_utility_estimates(agent, sequential_decision_environment, 500, [(2,2), (3,2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Active Reinforcement Learning\n",
    "\n",
    "Unlike Passive Reinforcement Learning in Active Reinforcement Learning we are not bound by a policy $\\pi$ and we need to select our actions. In other words the agent needs to learn an optimal policy. The fundamental tradeoff the agent needs to face is that of exploration vs. exploitation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QLearning Agent\n",
    "\n",
    "The `QLearningAgent` class implements the Agent Program described in **Fig 22.8** of the AIMA Book. In Q-Learning the agent learns an action-value function Q which gives the utility of taking a given action in a particular state. Q-Learning does not require a transition model and hence is a model free method. Let us look into its pseudocode before we implement our own version of this algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudocode(\"Q-Learning-Agent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Agent Program can be obtained by creating the instance of the class by passing the appropriate parameters. Because of the ``__call__`` method the object that is created behaves like a callable and returns an appropriate action as most Agent Programs do. To instantiate the object we need an MDP similar to the `PassiveTDAgent`. Let us use the same GridMDP object we used above. \n",
    "\n",
    "The `QLearningAgent` class also implements an exploration function **`f`** which returns fixed `Rplus` ($R^{+}$) until agent has visited state, action **`Ne`** ($N_{e}$) number of times. This is the same as the one defined for our optimistic initialisation. The method **`actions_in_state`** returns actions possible in given state. It is useful when applying `max` and `argmax` operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    \"\"\"\n",
    "    [Figure 21.8]\n",
    "    An exploratory Q-learning agent. It avoids having to learn the transition\n",
    "    model because the Q-value of a state can be related directly to those of\n",
    "    its neighbors.\n",
    "\n",
    "    import sys\n",
    "    from mdp import sequential_decision_environment\n",
    "    north = (0, 1)\n",
    "    south = (0,-1)\n",
    "    west = (-1, 0)\n",
    "    east = (1, 0)\n",
    "    policy = {(0, 2): east, (1, 2): east, (2, 2): east, (3, 2): None, (0, 1): north, (2, 1): north,\n",
    "              (3, 1): None, (0, 0): north, (1, 0): west, (2, 0): west, (3, 0): west,}\n",
    "    q_agent = QLearningAgent(sequential_decision_environment, Ne=5, Rplus=2, alpha=lambda n: 60./(59+n))\n",
    "    for i in range(200):\n",
    "        run_single_trial(q_agent,sequential_decision_environment)\n",
    "\n",
    "    q_agent.Q[((0, 1), (0, 1))] >= -0.5\n",
    "    True\n",
    "    q_agent.Q[((1, 0), (0, -1))] <= 0.5\n",
    "    True\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mdp, Ne, Rplus, alpha=None):\n",
    "\n",
    "        self.gamma = mdp.gamma\n",
    "        self.terminals = mdp.terminals\n",
    "        self.all_act = mdp.actlist\n",
    "        self.Ne = Ne  # iteration limit in exploration function\n",
    "        self.Rplus = Rplus  # large value to assign before iteration limit\n",
    "        self.Q = defaultdict(float)\n",
    "        self.Nsa = defaultdict(float)\n",
    "        self.s = None\n",
    "        self.a = None\n",
    "        self.r = None\n",
    "\n",
    "        if alpha:\n",
    "            self.alpha = alpha\n",
    "        else:\n",
    "            self.alpha = lambda n: 1. / (1 + n)  # udacity video\n",
    "\n",
    "    def f(self, u, n):\n",
    "        \"\"\"Exploration function. Returns fixed Rplus until\n",
    "        agent has visited state, action a Ne number of times.\n",
    "        Same as ADP agent in book.\"\"\"\n",
    "        if n < self.Ne:\n",
    "            return self.Rplus\n",
    "        else:\n",
    "            return u\n",
    "\n",
    "    def actions_in_state(self, state):\n",
    "        \"\"\"Return actions possible in given state.\n",
    "        Useful for max and argmax.\"\"\"\n",
    "        if state in self.terminals:\n",
    "            return [None]\n",
    "        else:\n",
    "            return self.all_act\n",
    "\n",
    "    def __call__(self, percept):\n",
    "        s1, r1 = self.update_state(percept)\n",
    "        Q, Nsa, s, a, r = self.Q, self.Nsa, self.s, self.a, self.r\n",
    "        alpha, gamma, terminals = self.alpha, self.gamma, self.terminals,\n",
    "        actions_in_state = self.actions_in_state\n",
    "\n",
    "        #### YOUR CODE HERE ####\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        #########################\n",
    "        return self.a\n",
    "\n",
    "    def update_state(self, percept):\n",
    "        \"\"\"To be overridden in most cases. The default case\n",
    "        assumes the percept to be of type (state, reward).\"\"\"\n",
    "        return percept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create our object now. We also use the **same alpha** as  before. We use **`Rplus = 2`** and **`Ne = 5`** as defined for **Fig 22.7**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_agent = QLearningAgent(sequential_decision_environment, Ne=5, Rplus=2, \n",
    "                         alpha=lambda n: 60./(59+n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to try out the q_agent we use the **`run_single_trial`** function (which was also used above). Let us use **200** iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(200)):\n",
    "    run_single_trial(q_agent,sequential_decision_environment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us see the Q Values. The keys are state-action pairs. Where different actions correspond according to:\n",
    "\n",
    "north = (0, 1)\n",
    "south = (0,-1)\n",
    "west = (-1, 0)\n",
    "east = (1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_agent.Q\n",
    "print('\\n'.join([str(k)+':'+str(v) for k, v in q_agent.Q.items()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Utility **U** of each state is related to **Q** by the following equation.\n",
    "\n",
    "$U(s) = \\max_{a}Q(s, a)$\n",
    "\n",
    "Let us convert the Q Values above into U estimates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U = defaultdict(lambda: -1000.) # Very Large Negative Value for Comparison see below.\n",
    "for state_action, value in q_agent.Q.items():\n",
    "    state, action = state_action\n",
    "    if U[state] < value:\n",
    "                U[state] = value\n",
    "print('\\n'.join([str(k)+':'+str(v) for k, v in U.items()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us finally compare these estimates to `value_iteration` results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Uvi = value_iteration(sequential_decision_environment)\n",
    "print('\\n'.join([str(k)+':'+str(v) for k, v in Uvi.items()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, let us compare the resulting policies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mdp4e import best_policy\n",
    "\n",
    "\n",
    "mdp = sequential_decision_environment\n",
    "\n",
    "mdp.display_policy(best_policy(mdp,U))\n",
    "mdp.display_policy(best_policy(mdp,Uvi))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "a04fbcb04a11c3a691703ca23ae4a78f9f717b538476169d44c620972eb30c1e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
