{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/abdn-cs3033-ai/practicals/blob/main/week08/tutorial7-mdp.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS3033: Artificial Intelligence\n",
    "\n",
    "## Tutorial 07: Markov Decision Processes\n",
    "\n",
    "#### Prof. Felipe Meneguzzi\n",
    "\n",
    "Adapted from code in the [AIMA-Python](https://github.com/aimacode/aima-python) public repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to run this tutorial, you need to download the auxiliary files from Github into your notebook, which we do with Jupyter's shell commands (if you downloaded the entire repo, the code below is not necessary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  import google.colab\n",
    "  print(\"We are in Google colab, we need to clone the repo\")\n",
    "  !pip3 install seaborn --user\n",
    "  !git clone https://github.com/abdn-cs3033-ai/practicals.git\n",
    "  %cd practicals/week08\n",
    "except:\n",
    "  print(\"Not in colab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Decision Processes\n",
    "\n",
    "**Markov Decision Processes** are stochastic processes in which an agent partially controls transitions between states by choosing actions, and which the subsequent transition probabilities between states follow the Markovian property. This means that single states include all the information relevant to compute the transition probability to other states. \n",
    "\n",
    "Recall the elements of an MDP:\n",
    "\n",
    "- A finite set of states $\\mathcal{S}$\n",
    "- A finite set of actions $\\mathcal{A}$\n",
    "- A *markovian* transition model $T(s,a,s') = \\mathbb{P}(S_{t+1}=s' \\mid S_t = s, A_t = a)$ \n",
    "- A reward function $R(s)$, alternatively $R(s,a) = \\mathbb{E}[ R_{t+1} \\mid S_t = s, A_t = a]$\n",
    "- A discount factor $\\gamma$\n",
    "\n",
    "We will implement MDPs in the **`MDP`** class below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from utils4e import vector_add, orientations, turn_right, turn_left\n",
    "\n",
    "from notebook import psource, pseudocode\n",
    "\n",
    "\n",
    "class MDP:\n",
    "    \"\"\"A Markov Decision Process, defined by an initial state, transition model,\n",
    "    and reward function. We also keep track of a gamma value, for use by\n",
    "    algorithms. The transition model is represented somewhat differently from\n",
    "    the text. Instead of P(s' | s, a) being a probability number for each\n",
    "    state/state/action triplet, we instead have T(s, a) return a\n",
    "    list of (p, s') pairs. We also keep track of the possible states,\n",
    "    terminal states, and actions for each state. [Page 646]\"\"\"\n",
    "\n",
    "    def __init__(self, init, actlist, terminals, transitions=None, reward=None, states=None, gamma=0.9):\n",
    "        if not (0 < gamma <= 1):\n",
    "            raise ValueError(\"An MDP must have 0 < gamma <= 1\")\n",
    "\n",
    "        # collect states from transitions table if not passed.\n",
    "        self.states = states or self.get_states_from_transitions(transitions)\n",
    "\n",
    "        self.init = init\n",
    "\n",
    "        if isinstance(actlist, list):\n",
    "            # if actlist is a list, all states have the same actions\n",
    "            self.actlist = actlist\n",
    "\n",
    "        elif isinstance(actlist, dict):\n",
    "            # if actlist is a dict, different actions for each state\n",
    "            self.actlist = actlist\n",
    "\n",
    "        self.terminals = terminals\n",
    "        self.transitions = transitions or {}\n",
    "        if not self.transitions:\n",
    "            print(\"Warning: Transition table is empty.\")\n",
    "\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.reward = reward or {s: 0 for s in self.states}\n",
    "\n",
    "        # self.check_consistency()\n",
    "\n",
    "    def R(self, state):\n",
    "        \"\"\"Return a numeric reward for this state.\"\"\"\n",
    "        if state not in self.reward: return None\n",
    "        return self.reward[state]\n",
    "\n",
    "    def T(self, state, action):\n",
    "        \"\"\"Transition model. From a state and an action, return a list\n",
    "        of (probability, result-state) pairs.\"\"\"\n",
    "\n",
    "        if not self.transitions:\n",
    "            raise ValueError(\"Transition model is missing\")\n",
    "        else:\n",
    "            return self.transitions[state][action]\n",
    "\n",
    "    def actions(self, state):\n",
    "        \"\"\"Return a list of actions that can be performed in this state. By default, a\n",
    "        fixed list of actions, except for terminal states. Override this\n",
    "        method if you need to specialize by state.\"\"\"\n",
    "\n",
    "        if state in self.terminals:\n",
    "            return [None]\n",
    "        else:\n",
    "            return self.actlist\n",
    "\n",
    "    def get_states_from_transitions(self, transitions):\n",
    "        if isinstance(transitions, dict):\n",
    "            s1 = set(transitions.keys())\n",
    "            s2 = set(tr[1] for actions in transitions.values()\n",
    "                     for effects in actions.values()\n",
    "                     for tr in effects)\n",
    "            return s1.union(s2)\n",
    "        else:\n",
    "            print('Could not retrieve states from transitions')\n",
    "            return None\n",
    "\n",
    "    def check_consistency(self):\n",
    "\n",
    "        # check that all states in transitions are valid\n",
    "        assert set(self.states) == self.get_states_from_transitions(self.transitions)\n",
    "\n",
    "        # check that init is a valid state\n",
    "        assert self.init in self.states\n",
    "\n",
    "        # check reward for each state\n",
    "        assert set(self.reward.keys()) == set(self.states)\n",
    "\n",
    "        # check that all terminals are valid states\n",
    "        assert all(t in self.states for t in self.terminals)\n",
    "\n",
    "        # check that probability distributions for all actions sum to 1\n",
    "        for s1, actions in self.transitions.items():\n",
    "            for a in actions.keys():\n",
    "                s = 0\n",
    "                for o in actions[a]:\n",
    "                    s += o[0]\n",
    "                assert abs(s - 1) < 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class above has attributes for the transition model, the reward function and other elements of an MDP.\n",
    "\n",
    "The **`__init__`** method takes in the following parameters:\n",
    "\n",
    "- `init`: the initial state.\n",
    "- `actlist`: List of actions possible in each state.\n",
    "- `terminals`: List of terminal states where only possible action is exit\n",
    "- `gamma`: Discounting factor $\\gamma$. This makes sure that delayed rewards have less value compared to immediate ones.\n",
    "\n",
    "The **`R`** method returns the reward for each state by using the `self.reward` `dict`. This corresponds to the $R(s)$ formulation.\n",
    "\n",
    "The **`T`** method is somewhat different from the text. Here we return (probability, $s'$) pairs where $s'$ belongs to list of possible state by taking action $a$ in state $s$.\n",
    "\n",
    "The **`actions`** method returns list of actions possible in each state. By default, it returns all actions for states other than terminal states.\n",
    "\n",
    "Note that we also import a number of auxiliary functions you will need in implementing the algorithms we require, specifically:\n",
    "\n",
    "- ```vector_add```\n",
    "- ```orientations```\n",
    "- ```turn_right```\n",
    "- ```turn_left```\n",
    "\n",
    "For this tutorial, we will use a specific subclass of the `MDP` we just created that we specialise to represent the grid worlds we saw in the lecture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "\n",
    "class GridMDP(MDP):\n",
    "    \"\"\"A two-dimensional grid MDP, as in [Figure 16.1]. All you have to do is\n",
    "    specify the grid as a list of lists of rewards; use None for an obstacle\n",
    "    (unreachable state). Also, you should specify the terminal states.\n",
    "    An action is an (x, y) unit vector; e.g. (1, 0) means move east.\"\"\"\n",
    "\n",
    "    def __init__(self, grid, terminals, init=(0, 0), gamma=.9):\n",
    "        grid.reverse()  # because we want row 0 on bottom, not on top\n",
    "        reward = {}\n",
    "        states = set()\n",
    "        self.rows = len(grid)\n",
    "        self.cols = len(grid[0])\n",
    "        self.grid = grid\n",
    "        for x in range(self.cols):\n",
    "            for y in range(self.rows):\n",
    "                if grid[y][x]:\n",
    "                    states.add((x, y))\n",
    "                    reward[(x, y)] = grid[y][x]\n",
    "        self.states = states\n",
    "        actlist = orientations\n",
    "        transitions = {}\n",
    "        for s in states:\n",
    "            transitions[s] = {}\n",
    "            for a in actlist:\n",
    "                transitions[s][a] = self.calculate_T(s, a)\n",
    "        MDP.__init__(self, init, actlist=actlist,\n",
    "                     terminals=terminals, transitions=transitions,\n",
    "                     reward=reward, states=states, gamma=gamma)\n",
    "\n",
    "    def calculate_T(self, state, action):\n",
    "        if action:\n",
    "            return [(0.8, self.go(state, action)),\n",
    "                    (0.1, self.go(state, turn_right(action))),\n",
    "                    (0.1, self.go(state, turn_left(action)))]\n",
    "        else:\n",
    "            return [(0.0, state)]\n",
    "\n",
    "    def T(self, state, action):\n",
    "        return self.transitions[state][action] if action else [(0.0, state)]\n",
    "\n",
    "    def go(self, state, direction):\n",
    "        \"\"\"Return the state that results from going in this direction.\"\"\"\n",
    "\n",
    "        state1 = tuple(vector_add(state, direction))\n",
    "        return state1 if state1 in self.states else state\n",
    "\n",
    "    def to_grid(self, mapping, default = None):\n",
    "        \"\"\"Convert a mapping from (x, y) to v into a [[..., v, ...]] grid.\"\"\"\n",
    "\n",
    "        return list(reversed([[mapping.get((x, y), default)\n",
    "                               for x in range(self.cols)]\n",
    "                              for y in range(self.rows)]))\n",
    "\n",
    "    def to_arrows(self, policy):\n",
    "        chars = {(1, 0): '>', (0, 1): '^', (-1, 0): '<', (0, -1): 'v', None: '.'}\n",
    "        return self.to_grid({s: chars[a] for (s, a) in policy.items()})\n",
    "    \n",
    "    def display_policy(self, policy):\n",
    "        \"\"\"Pretty prints the policy in HTML for a Jupyter Notebook\"\"\"\n",
    "        # SYMBOLS = ['&rarr;', '&uarr;', '&larr;', '&darr;']\n",
    "        SYMBOLS = {(1, 0): '&rarr;', (0, 1): '&uarr;', (-1, 0): '&larr;', (0, -1): '&darr;', None: '&#x25CE;'}\n",
    "\n",
    "        p_policy = self.to_grid({s: SYMBOLS[a] for (s, a) in policy.items()}, '&#x25FE;')\n",
    "\n",
    "        display(HTML(\n",
    "        '<table style=\"font-size:300%;border: thick solid;\"><tr>{}</tr></table>'.format(\n",
    "            '</tr><tr>'.join(\n",
    "                '<td>{}</td>'.format('</td><td>'.join(str(_) for _ in row)) for row in p_policy))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have defined the class for grids, we can represent examples such as the one we saw in the lecture, reproduced below.\n",
    "\n",
    "![Grid World](img/mdp-bare.svg \"Grid World MDP illustration\")\n",
    "\n",
    "We instantiate the object **`mdp`** of the class using a list of lists for both the transition and the sensor model. The code below instantiates the Grid World shown above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp = GridMDP([[-3, -3, -3, +100],\n",
    "            [-3, None,  -3, -100],\n",
    "            [-3, -3, -3, -3]],\n",
    "            terminals=[(3, 2), (3, 1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The **__init__** method takes **grid** as an extra parameter compared to the MDP class. The grid is a nested list of rewards in states.\n",
    "\n",
    "The **`go`** method returns the state by going in particular direction by using `vector_add`.\n",
    "\n",
    "The **`actions`** method returns list of actions possible in each state. By default, it returns all actions for states other than terminal states.\n",
    "\n",
    "**`to_arrows`** are used for representing the policy in a grid-like text format. By contrast `display_policy` pretty prints the policy in HTML within the notebook.\n",
    "\n",
    "The **`T`** method is somewhat different from the text. Here we return (probability, $s'$) pairs where $s'$ belongs to list of possible state by taking action $a$ in state $s$. Thus **`T(state, action)`** yields the transition model of the MDP for a given state and action. Note that the indexes in our MDP start in zero.\n",
    "\n",
    "Similarly, the **`R(state)`** method returns the immediate reward for a state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils4e import NORTH\n",
    "print(mdp.T((2,2), NORTH))\n",
    "print(mdp.R((3,2)))\n",
    "print(mdp.R((1,1)))\n",
    "print(mdp.states)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving MDPs\n",
    "\n",
    "Now that we have looked how to represent MDPs. We will implement algorithms to solve them. Our ultimate goal is to obtain an optimal policy. We start with the Value Iteration algorithm and a visualisation that should help us understand it better.\n",
    "\n",
    "We start by calculating Value/Utility for each of the states. The Value of each state is the expected sum of discounted future rewards given we start in that state and follow a particular policy $\\pi$. The value or the utility of a state is given by:\n",
    "\n",
    "$$U(s)=R(s)+\\gamma\\max_{a\\epsilon A(s)}\\sum_{s'} P(s'\\ |\\ s,a)U(s')$$\n",
    "\n",
    "This is called the Bellman equation. The Value Iteration algorithm (**Fig. 17.4** in the book) relies on finding solutions of this Equation. The intuition Value Iteration works is because values propagate through the state space by means of local updates. This point will we more clear after we encounter the visualisation. For more information you can refer to **Section 17.2** of the book. \n",
    "\n",
    "Let us first review the pseudocode for value iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudocode(\"Value-Iteration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Iteration\n",
    "Using the pseudocode above we will implement the value iteration algorithm. \n",
    "\n",
    "Our implementation takes as inputs two parameters, an MDP to solve and epsilon, the maximum error allowed in the utility of any state. It returns a dictionary containing utilities where the keys are the states and values represent utilities. \n",
    "\n",
    "Value Iteration starts with arbitrary initial values for the utilities, calculates the right-hand side of the Bellman equation and plugs it into the left-hand side, thereby updating the utility of each state from the utilities of its neighbours. \n",
    "This is repeated until the utility vector we update reaches an equilibrium. \n",
    "It works on the principle of _Dynamic Programming_ - using precomputed information to simplify the subsequent computation. \n",
    "If $U_i(s)$ is the utility value for state $s$ at the $i$ th iteration, the iteration step, called Bellman update, looks like this:\n",
    "\n",
    "$$ U_{i+1}(s) \\leftarrow R(s) + \\gamma \\max_{a \\epsilon A(s)} \\sum_{s'} P(s'\\ |\\ s,a)U_{i}(s') $$\n",
    "\n",
    "As you might have noticed, `value_iteration` has an infinite loop. How do we decide when to stop iterating? \n",
    "The concept of _contraction_ successfully explains the convergence of value iteration. \n",
    "Refer to **Section 17.2.3** of the book for a detailed explanation. \n",
    "In the algorithm, we calculate a value $delta$ that measures the difference in the utilities of the current time step and the previous time step. \n",
    "\n",
    "$$\\delta = \\max{(\\delta, \\begin{vmatrix}U_{i + 1}(s) - U_i(s)\\end{vmatrix})}$$\n",
    "\n",
    "This value of delta decreases as the values of $U_i$ converge.\n",
    "We terminate the algorithm if the $\\delta$ value is less than a threshold value determined by the hyperparameter _epsilon_.\n",
    "\n",
    "$$\\delta \\lt \\epsilon \\frac{(1 - \\gamma)}{\\gamma}$$\n",
    "\n",
    "To summarize, the Bellman update is a _contraction_ by a factor of $gamma$ on the space of utility vectors. \n",
    "Hence, from the properties of contractions in general, it follows that `value_iteration` always converges to a unique solution of the Bellman equations whenever $gamma$ is less than 1.\n",
    "We then terminate the algorithm when we reach a reasonable approximation.\n",
    "In practice, it often occurs that the policy $pi$ becomes optimal long before the utility function converges. For the given 4 x 3 environment with $gamma = 0.9$, the policy $pi$ is optimal when $i = 4$ (at the 4th iteration), even though the maximum error in the utility function is stil 0.46. This can be clarified from **figure 17.6** in the book. Hence, to increase computational efficiency, we often use another method to solve MDPs called Policy Iteration which we will see in the later part of this notebook. \n",
    "\n",
    "In this tutorial, you only need to implement the Bellman update within the algorithm.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ______________________________________________________________________________\n",
    "# 16.1.3 The Bellman equation for utilities\n",
    "\n",
    "### You can use this function in your value iteration implementation (but you don't need it)\n",
    "def q_value(mdp, s, a, U):\n",
    "    \"\"\"Returns the expected value of taking an action `a` at a state `s` under utility `U`\"\"\"\n",
    "    if not a:\n",
    "        return mdp.R(s)\n",
    "    res = 0\n",
    "    for p, s_prime in mdp.T(s, a):\n",
    "        res += p * (mdp.R(s) + mdp.gamma * U[s_prime])\n",
    "    return res\n",
    "\n",
    "def value_iteration(mdp, epsilon=0.001):\n",
    "    \"\"\"Solving an MDP by value iteration. [Figure 16.6]\"\"\"\n",
    "\n",
    "    U1 = {s: 0 for s in mdp.states}\n",
    "    R, T, gamma = mdp.R, mdp.T, mdp.gamma\n",
    "    while True:\n",
    "        U = U1.copy()\n",
    "        delta = 0\n",
    "        for s in mdp.states:\n",
    "            ### Your code here\n",
    "            \n",
    "\n",
    "            \n",
    "            ### End code\n",
    "            delta = max(delta, abs(U1[s] - U[s]))\n",
    "        if delta <= epsilon * (1 - gamma) / gamma:\n",
    "            return U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will notice that the `value_iteration` function above does not return a policy, just the discounted returns for each state stored in the `U` data structure. \n",
    "\n",
    "In order to extract a policy out of these values, we need a function that selects the action with the best expected reward for each state, essentially implementing the policy extraction we saw in the lecture:\n",
    "\n",
    "$$\\pi(s) = \\argmax_{a} \\sum_{s'}P(s' \\mid s,a)*V(s')$$\n",
    "\n",
    "This is what you will implement below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_policy(mdp, U):\n",
    "    \"\"\"Given an MDP and a utility function U, determine the best policy,\n",
    "    as a mapping from state to action.\"\"\"\n",
    "\n",
    "    pi = {}\n",
    "    for s in mdp.states:\n",
    "        ### Your code here\n",
    "\n",
    "        ### End code\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, let us solve the GridMDP we defined, using `value_iteration`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp = GridMDP([[-3, -3, -3, +100],\n",
    "            [-3, None,  -3, -100],\n",
    "            [-3, -3, -3, -3]],\n",
    "            terminals=[(3, 2), (3, 1)], gamma=1)\n",
    "U = value_iteration(mdp, epsilon=0.01)\n",
    "policy = best_policy(mdp,U)\n",
    "print(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the resulting policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp.display_policy(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Value Iteration\n",
    "\n",
    "To illustrate that values propagate out of states let us create a simple visualisation. We will be using a modified version of the ```value_iteration``` function which will store U over time. We will also remove the parameter epsilon and instead add the number of iterations we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration_instru(mdp, iterations=20):\n",
    "    U_over_time = []\n",
    "    U1 = {s: 0 for s in mdp.states}\n",
    "    R, T, gamma = mdp.R, mdp.T, mdp.gamma\n",
    "    for _ in range(iterations):\n",
    "        U = U1.copy()\n",
    "        for s in mdp.states:\n",
    "            U1[s] = R(s) + gamma * max([sum([p * U[s1] for (p, s1) in T(s, a)])\n",
    "                                        for a in mdp.actions(s)])\n",
    "        U_over_time.append(U)\n",
    "    return U_over_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a function to create the visualisation from the utilities returned by **value_iteration_instru**. Do not worry with the code that immediately follows as it is the usage of Matplotib with IPython Widgets. If you are interested in reading more about these visit [ipywidgets.readthedocs.io](http://ipywidgets.readthedocs.io)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = 4\n",
    "rows = 3\n",
    "U_over_time = value_iteration_instru(mdp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from notebook import make_plot_grid_step_function\n",
    "\n",
    "plot_grid_step = make_plot_grid_step_function(columns, rows, U_over_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from notebook import make_visualize\n",
    "\n",
    "iteration_slider = widgets.IntSlider(min=1, max=15, step=1, value=0)\n",
    "w=widgets.interactive(plot_grid_step,iteration=iteration_slider)\n",
    "display(w)\n",
    "\n",
    "visualize = True\n",
    "visualize_callback = make_visualize(iteration_slider)\n",
    "\n",
    "visualize_button = widgets.ToggleButton(description = \"Visualize\", value = False)\n",
    "time_select = widgets.ToggleButtons(description='Extra Delay:',options=['0', '0.1', '0.2', '0.5', '0.7', '1.0'])\n",
    "a = widgets.interactive(visualize_callback, Visualize = visualize_button, time_step=time_select)\n",
    "display(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move the slider above to observe how the utility changes across iterations. It is also possible to move the slider using arrow keys or to jump to the value by directly editing the number with a double click. The **Visualize Button** will automatically animate the slider for you. The **Extra Delay Box** allows you to set time delay in seconds up to one second for each time step. \n",
    "\n",
    "### Effects of the discount factor\n",
    "\n",
    "Here we plot the difference between each iteration of the Value Iteration algorithm using different discount factors. The closest the $\\gamma$ is to zero, the more the values contract over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_delta(mdp, discounts=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95], epsilon=0.001, max_iter=1000):\n",
    "    data_list = []\n",
    "    import pandas as pd\n",
    "    from matplotlib import pyplot as plt\n",
    "    %matplotlib inline\n",
    "    import seaborn as sns\n",
    "    data_list = []\n",
    "    for d in discounts:\n",
    "        mdp.gamma = d\n",
    "        U_over_time = value_iteration_instru(mdp)\n",
    "        for i in range(len(U_over_time)):\n",
    "            if i > 0:\n",
    "                v = np.array(list(U_over_time[i-1].values()))\n",
    "                vprime = np.array(list(U_over_time[i].values()))\n",
    "                value = max(abs(vprime - v))\n",
    "                data_list.append([value, 'gamma: ' + str(d), i])\n",
    "    data_frame2 = pd.DataFrame(data_list, columns=['delta', 'discount', 'iterations'])\n",
    "    ax = sns.relplot(x = 'iterations', y = 'delta', hue='discount', kind=\"line\", data=data_frame2)\n",
    "mdp = GridMDP([[-3, -3, -3, +100],\n",
    "            [-3, None,  -3, -100],\n",
    "            [-3, -3, -3, -3]],\n",
    "            terminals=[(3, 2), (3, 1)])\n",
    "plot_delta(mdp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Iteration\n",
    "\n",
    "We have already seen that value iteration converges to the optimal policy long before it accurately estimates the utility function. \n",
    "If one action is clearly better than all the others, then the exact magnitude of the utilities in the states involved need not be precise. \n",
    "The policy iteration algorithm works on this insight. \n",
    "The algorithm executes two fundamental steps:\n",
    "\n",
    "- **Policy evaluation**: Given a policy _&#960;&#7522;_, calculate _U&#7522; = U(&#960;&#7522;)_, the utility of each state if _&#960;&#7522;_ were to be executed.\n",
    "- **Policy improvement**: Calculate a new policy _&#960;&#7522;&#8330;&#8321;_ using one-step look-ahead based on the utility values calculated.\n",
    "\n",
    "The algorithm terminates when the policy improvement step yields no change in the utilities. \n",
    "Refer to **Figure 17.6** in the book to see how this is an improvement over value iteration.\n",
    "We now have a simplified version of the Bellman equation:\n",
    "\n",
    "$$U_i(s) = R(s) + \\gamma \\sum_{s'}P(s'\\ |\\ s, \\pi_i(s))U_i(s')$$\n",
    "\n",
    "An important observation in this equation is that this equation does not have the `max` operator, which makes it linear.\n",
    "For _n_ states, we have _n_ linear equations with _n_ unknowns, which can be solved exactly in time _**O(n&#179;)**_.\n",
    "For more implementational details, have a look at **Section 17.3**.\n",
    "\n",
    "Let us now look at how we can compute the expected utility and how we can implement `policy_iteration`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_utility(a, s, U, mdp):\n",
    "    \"\"\"The expected utility of doing a in state s, according to the MDP and U.\"\"\"\n",
    "    ### Your code here\n",
    "\n",
    "    ### End code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can test the algorithm with the example in the book for the square to the left of the positive reward terminal state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp = GridMDP([[-3, -3, -3, +100],\n",
    "        [-3, None,  -3, -100],\n",
    "        [-3, -3, -3, -3]],\n",
    "        terminals=[(3, 2), (3, 1)])\n",
    "\n",
    "U = value_iteration(mdp)\n",
    "\n",
    "EAST, NORTH, WEST, SOUTH = [(1, 0), (0, 1), (-1, 0), (0, -1)]\n",
    "\n",
    "## Check that we computed this correctly\n",
    "assert(abs(expected_utility(EAST, (2,2), U, mdp) - 93.2) < 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not need to do _exact_ policy evaluation. \n",
    "We can instead approximate them reasonably by performing some number of simplified value iteration steps.\n",
    "The simplified Bellman update equation for the process is:\n",
    "\n",
    "$$U_{i+1}(s) \\leftarrow R(s) + \\gamma\\sum_{s'}P(s'\\ |\\ s,\\pi_i(s))U_{i}(s')$$\n",
    "\n",
    "which we repeat _k_ times to produce the next utility estimate. This is called _modified policy iteration_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudocode('Policy-Iteration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(mdp):\n",
    "    \"\"\"Solve an MDP by policy iteration [Figure 17.7]\"\"\"\n",
    "\n",
    "    U = {s: 0 for s in mdp.states}\n",
    "    pi = {s: random.choice(mdp.actions(s)) for s in mdp.states}\n",
    "    while True:\n",
    "        U = policy_evaluation(pi, U, mdp)\n",
    "        unchanged = True\n",
    "        for s in mdp.states:\n",
    "            a_star = max(mdp.actions(s), key=lambda a: q_value(mdp, s, a, U))\n",
    "            # a = max(mdp.actions(s), key=lambda a: expected_utility(a, s, U, mdp))\n",
    "            if q_value(mdp, s, a_star, U) > q_value(mdp, s, pi[s], U):\n",
    "                pi[s] = a_star\n",
    "                unchanged = False\n",
    "        if unchanged:\n",
    "            return pi\n",
    "\n",
    "\n",
    "def policy_evaluation(pi, U, mdp, k=20):\n",
    "    \"\"\"Return an updated utility mapping U from each state in the MDP to its\n",
    "    utility, using an approximation (modified policy iteration).\"\"\"\n",
    "\n",
    "    R, T, gamma = mdp.R, mdp.T, mdp.gamma\n",
    "    for i in range(k):\n",
    "        for s in mdp.states:\n",
    "            U[s] = R(s) + gamma * sum(p * U[s1] for (p, s1) in T(s, pi[s]))\n",
    "    return U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now test this new algorithm with our Grid World."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp = GridMDP([[-3, -3, -3, +100],\n",
    "        [-3, None,  -3, -100],\n",
    "        [-3, -3, -3, -3]],\n",
    "        terminals=[(3, 2), (3, 1)], gamma=1)\n",
    "\n",
    "pi = policy_iteration(mdp)\n",
    "print(\"This is the policy from Policy Iteration\")\n",
    "mdp.display_policy(pi)\n",
    "# As well as compare the result of computing policies using value iteration for comparison\n",
    "\n",
    "Uvi = value_iteration(mdp)\n",
    "piVi = best_policy(mdp, Uvi)\n",
    "print(\"This is the policy from Value Iteration\")\n",
    "mdp.display_policy(piVi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try to reproduce all the cases we saw in the lecture for different immediate rewards in the code below, see how they look like.\n",
    "\n",
    "![Sequential Decision Policies](img/sequential-decision-policies.svg \"Sequential Decision Policies\")\n",
    "\n",
    "#### Case 1\n",
    "\n",
    "Notice that, because the cost of taking a step is fairly small compared with the penalty for ending up in `(4, 2)` by accident, the optimal policy is conservative. \n",
    "In state `(3, 1)` it recommends taking the long way round, rather than taking the shorter way and risking getting a large negative reward of -1 in `(4, 2)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that this environment is also initialized in mdp.py by default\n",
    "ntr = -0.04 # setting the non-terminal reward\n",
    "mdp = GridMDP([[ntr, ntr, ntr, +1],\n",
    "                [ntr, None, ntr, -1],\n",
    "                [ntr, ntr, ntr, ntr]],\n",
    "                terminals=[(3, 2), (3, 1)], gamma=1)\n",
    "\n",
    "pi = best_policy(mdp, value_iteration(mdp, .001))\n",
    "\n",
    "mdp.display_policy(pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case 2\n",
    "$R(s) = -0.4$ in all states except in terminal states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntr = -0.4 # setting the non-terminal reward\n",
    "mdp = GridMDP([[ntr, ntr, ntr, +1],\n",
    "                [ntr, None, ntr, -1],\n",
    "                [ntr, ntr, ntr, ntr]],\n",
    "                terminals=[(3, 2), (3, 1)])\n",
    "\n",
    "pi = best_policy(mdp, value_iteration(mdp, .001), gamma=1)\n",
    "\n",
    "mdp.display_policy(pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the reward for each state is now more negative, life is certainly more unpleasant.\n",
    "The agent takes the shortest route to the $+1$ state and is willing to risk falling into the $-1$ state by accident."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case 3\n",
    "\n",
    "$R(s) = -4$ in all states except terminal states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntr = -4 # setting the non-terminal reward\n",
    "mdp = GridMDP([[ntr, ntr, ntr, +1],\n",
    "                [ntr, None, ntr, -1],\n",
    "                [ntr, ntr, ntr, ntr]],\n",
    "                terminals=[(3, 2), (3, 1)], gamma=1)\n",
    "\n",
    "pi = best_policy(mdp, value_iteration(mdp, .001))\n",
    "\n",
    "mdp.display_policy(pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The living reward for each state is now lower than the least rewarding terminal. Life is so _painful_ that the agent heads for the nearest exit as even the worst exit is less painful than any living state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case 4\n",
    "\n",
    "$R(s) = 4$ in all states except terminal states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntr = 4 # setting the non-terminal reward\n",
    "mdp = GridMDP([[ntr, ntr, ntr, +1],\n",
    "                [ntr, None, ntr, -1],\n",
    "                [ntr, ntr, ntr, ntr]],\n",
    "                terminals=[(3, 2), (3, 1)], gamma=0.99)\n",
    "\n",
    "pi = best_policy(mdp, value_iteration(mdp, .001))\n",
    "\n",
    "mdp.display_policy(pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As life is positively enjoyable, the agent avoids _both_ exits.\n",
    "Even though the output we get is not exactly what we want, it is definitely not wrong.\n",
    "The scenario here requires the agent to anything but reach a terminal state, as this is the only way the agent can maximize its reward (total reward tends to infinity), and the policy does just that.\n",
    "\n",
    "\n",
    "Currently, the GridMDP class does not support an explicit marker for a \"do whatever you like\" action or a \"don't care\" condition.\n",
    "\n",
    "You can however, extend the class to do so.\n",
    "\n",
    "\n",
    "For in-depth knowledge about sequential decision problems, refer **Section 17.1** in the AIMA book."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15 (default, Nov 24 2022, 12:02:37) \n[Clang 14.0.6 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "a04fbcb04a11c3a691703ca23ae4a78f9f717b538476169d44c620972eb30c1e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
