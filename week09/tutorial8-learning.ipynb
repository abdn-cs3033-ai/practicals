{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/abdn-cs3033-ai/practicals/blob/main/week09/tutorial8-learning.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS3033: Artificial Intelligence\n",
    "\n",
    "## Tutorial 08: Supervised Learning\n",
    "\n",
    "#### Prof. Felipe Meneguzzi\n",
    "\n",
    "Adapted from code in the [AIMA-Python](https://github.com/aimacode/aima-python) public repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to run this tutorial, you need to download the auxiliary files from Github into your notebook, which we do with Jupyter's shell commands (if you downloaded the entire repo, the code below is not necessary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  import google.colab\n",
    "  print(\"We are in Google colab, we need to clone the repo\")\n",
    "  !git clone https://github.com/abdn-cs3033-ai/practicals.git\n",
    "  %cd practicals/week09\n",
    "  %pip install -r requirements.txt\n",
    "except:\n",
    "  print(\"Not in colab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "The key ingredient for machine learning algorithms is Data. Thus, we start by reviewing a Dataset class for transparent access from our learning algorithms. \n",
    "A lot of the datasets we will work with are .csv files (although other formats are supported too). There is a collection of sample datasets ready to use [on aima-data](https://github.com/aimacode/aima-data), and we use a subset we placed on [our branch](./aima-data). In such files, each line corresponds to one item/measurement. Each individual value in a line represents a *feature* and usually there is a value denoting the *class* of the item. We encode data using the [DataSet](learning.py#L8) class (which we import in the next Python cell).\n",
    "\n",
    "### Class Attributes\n",
    "\n",
    "* **examples**: Holds the items of the dataset. Each item is a list of values.\n",
    "\n",
    "* **attrs**: The indexes of the features (by default in the range of [0,f), where *f* is the number of features). For example, `item[i]` returns the feature at index *i* of *item*.\n",
    "\n",
    "* **attrnames**: An optional list with attribute names. For example, `item[s]`, where *s* is a feature name, returns the feature of name *s* in *item*.\n",
    "\n",
    "* **target**: The attribute a learning algorithm will try to predict. By default the last attribute.\n",
    "\n",
    "* **inputs**: This is the list of attributes without the target.\n",
    "\n",
    "* **values**: A list of lists which holds the set of possible values for the corresponding attribute/feature. If initially `None`, it gets computed (by the function `setproblem`) from the examples.\n",
    "\n",
    "* **distance**: The distance function used in the learner to calculate the distance between two items. By default `mean_boolean_error`.\n",
    "\n",
    "* **name**: Name of the dataset.\n",
    "\n",
    "* **source**: The source of the dataset (url or other). Not used in the code.\n",
    "\n",
    "* **exclude**: A list of indexes to exclude from `inputs`. The list can include either attribute indexes (attrs) or names (attrnames).\n",
    "\n",
    "<!-- - ```d.examples``` — A list of examples. Each one is a list of attribute values.\n",
    "- ```d.attrs```    — A list of integers to index into an example, so ```example[attr]``` gives a value. Normally the same as ```range(len(d.examples[0]))```.\n",
    "- ```d.attr_names``` — Optional list of mnemonic names for corresponding ```attrs```.\n",
    "- ```d.target``` — The attribute that a learning algorithm will try to predict. By default the final attribute.\n",
    "- ```d.inputs``` — The list of ```attrs``` without the target.\n",
    "- ```d.values``` — A list of lists: each sublist is the set of possible values for the corresponding attribute. If initially ```None```, it is computed from the known examples by ```self.set_problem```. If not ```None```, an erroneous value raises ```ValueError```.\n",
    "- ```d.distance``` A function from a pair of examples to a non-negative number. Should be symmetric, etc. Defaults to ```mean_boolean_error``` since that can handle any field types.\n",
    "- ```d.name``` — Name of the data set (for output display only).\n",
    "- ```d.source``` — URL or other source where the data came from.\n",
    "- ```d.exclude``` — A list of attribute indexes to exclude from d.inputs. Elements of this list can either be integers (```attrs```) or ```attr_names```. -->\n",
    "\n",
    "<!-- Normally, you call the constructor and you are done. Then you just access fields like `d.examples`, `d.target` and `d.inputs`. -->\n",
    "\n",
    "### Class Helper Functions\n",
    "\n",
    "These functions help modify a `DataSet` object to your needs.\n",
    "\n",
    "* **sanitize**: Takes as input an example and returns it with non-input (target) attributes replaced by `None`. Useful for testing. Keep in mind that the example given is not itself sanitized, but instead a sanitized copy is returned.\n",
    "\n",
    "* **classes_to_numbers**: Maps the class names of a dataset to numbers. If the class names are not given, they are computed from the dataset values. Useful for classifiers that return a numerical value instead of a string.\n",
    "\n",
    "* **remove_examples**: Removes examples containing a given value. Useful for removing examples with missing values, or for removing classes (needed for binary classifiers).\n",
    "\n",
    "With that infrastructure in place, we now instantiate the restaurant dataset from the book, and which we used in the lecture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from collections import defaultdict\n",
    "from statistics import stdev\n",
    "from utils4e import argmax_random_tie, normalize, remove_all\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from notebook import psource, pseudocode\n",
    "from learning import DataSet, parse_csv\n",
    "\n",
    "def RestaurantDataSet(examples=None):\n",
    "    \"\"\"\n",
    "    [Figure 18.3]\n",
    "    Build a DataSet of Restaurant waiting examples.\n",
    "    \"\"\"\n",
    "    return DataSet(name='restaurant', target='Wait', examples=examples,\n",
    "                   attr_names='Alternate Bar Fri/Sat Hungry Patrons Price Raining Reservation Type WaitEstimate Wait')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing a Dataset\n",
    "\n",
    "#### Importing from aima-data\n",
    "\n",
    "Datasets uploaded on aima-data can be imported with the following line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's check that this dataset worked\n",
    "\n",
    "restaurant1 = DataSet(name='restaurant')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the class we defined above, and to check that we imported the correct dataset, we can do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant = RestaurantDataSet()\n",
    "\n",
    "## Check the first example in the dataset\n",
    "\n",
    "print(restaurant.examples[0])\n",
    "print(restaurant.inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which correctly prints the first line in the csv file and the list of attribute indexes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When importing a dataset, we can specify to exclude an attribute (for example, at index 1) by setting the parameter `exclude` to the attribute index or name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the full dataset\n",
    "housing = DataSet(name='housing', target='Price', examples=None,\n",
    "                   attr_names='Size Bedrooms Price')\n",
    "\n",
    "print(housing.inputs)\n",
    "\n",
    "# Load the full dataset\n",
    "\n",
    "housing = DataSet(name='housing', target='Price', examples=None,\n",
    "                   attr_names='Size Bedrooms Price', exclude=[1])\n",
    "print(housing.inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attributes\n",
    "\n",
    "Here we showcase the attributes.\n",
    "\n",
    "First we will print the first three items/examples in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(restaurant.examples[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we will print `attrs`, `attr_names`, `target`, `input`. Notice how `attrs` holds values in [0,10], but since the fourth attribute is the target, `inputs` holds values in [0,9]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"attrs:\", restaurant.attrs)\n",
    "print(\"attrnames (by default same as attrs):\", restaurant1.attr_names)\n",
    "print(\"attrnames (by default same as attrs):\", restaurant.attr_names)\n",
    "print(\"target:\", restaurant.target)\n",
    "print(\"inputs:\", restaurant.inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will print all the possible values for the first feature/attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(restaurant.values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will print the dataset's name and source. Keep in mind that we have not set a source for the dataset, so in this case it is empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"name:\", restaurant.name)\n",
    "print(\"source:\", restaurant.source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A useful combination of the above is `dataset.values[dataset.target]` which returns the possible values of the target. For classification problems, this will return all the possible classes. Let's try it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(restaurant.values[restaurant.target])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions\n",
    "\n",
    "We will now take a look at the auxiliary functions found in the class.\n",
    "\n",
    "First we will take a look at the `sanitize` function, which sets the non-input values of the given example to `None`.\n",
    "\n",
    "In this case we want to hide the class of the first example, so we will sanitize it.\n",
    "\n",
    "Note that the function doesn't actually change the given example; it returns a sanitized *copy* of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sanitized:\",restaurant.sanitize(restaurant.examples[0]))\n",
    "print(\"Original:\",restaurant.examples[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have `classes_to_numbers`. For a lot of the classifiers (like Neural Networks and Logistic Regression/Classification), classes should have numerical values. With this function we map string class names to numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Class of first example:\",restaurant1.examples[0][restaurant1.target])\n",
    "restaurant1.classes_to_numbers()\n",
    "print(\"Class of first example:\",restaurant1.examples[0][restaurant1.target])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see \"Yes\" was mapped to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we take a look at `find_means_and_deviations`. It finds the means and standard deviations of the features for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO Debug this\n",
    "\n",
    "# means, deviations = restaurant1.find_means_and_deviations()\n",
    "\n",
    "# print(\"Yes target feature means:\", means[\"Yes\"])\n",
    "# print(\"No mean for first feature:\", means[\"No\"][0])\n",
    "\n",
    "# print(\"Yes target feature deviations:\", deviations[\"Yes\"])\n",
    "# print(\"No deviation for second feature:\",deviations[\"No\"][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees\n",
    "\n",
    "A decision tree is a flowchart that uses a tree of decisions and their possible consequences for classification. At each non-leaf node of the tree an attribute of the input is tested, based on which corresponding branch leading to a child-node is selected. At the leaf node the input is classified based on the class label of this leaf node. The paths from root to leaves represent classification rules based on which leaf nodes are assigned class labels.\n",
    "\n",
    "We now proceed to developing our algorithm for learning decision trees, for this, we will need a couple of data structures we define below, specifically, the decision points in the decision tree, and the leaf nodes. \n",
    "\n",
    "### Implementation\n",
    "The nodes of the tree constructed by our learning algorithm are stored using either `DecisionFork` or `DecisionLeaf` based on whether they are a parent node or a leaf node respectively.\n",
    "\n",
    "`DecisionFork` holds the attribute, which is tested at that node, and a dict of branches. The branches store the child nodes, one for each of the attribute's values. Calling an object of this class as a function with input tuple as an argument returns the next node in the classification path based on the result of the attribute test.\n",
    "\n",
    "The leaf node stores the class label in `result`. All input tuples' classification paths end on a `DecisionLeaf` whose `result` attribute decide their class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionFork:\n",
    "    \"\"\"\n",
    "    A fork of a decision tree holds an attribute to test, and a dict\n",
    "    of branches, one for each of the attribute's values.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, attr, attr_name=None, default_child=None, branches=None):\n",
    "        \"\"\"Initialize by saying what attribute this node tests.\"\"\"\n",
    "        self.attr = attr\n",
    "        self.attr_name = attr_name or attr\n",
    "        self.default_child = default_child\n",
    "        self.branches = branches or {}\n",
    "\n",
    "    def __call__(self, example):\n",
    "        \"\"\"Given an example, classify it using the attribute and the branches.\"\"\"\n",
    "        attr_val = example[self.attr]\n",
    "        if attr_val in self.branches:\n",
    "            return self.branches[attr_val](example)\n",
    "        else:\n",
    "            # return default class when attribute is unknown\n",
    "            return self.default_child(example)\n",
    "\n",
    "    def add(self, val, subtree):\n",
    "        \"\"\"Add a branch. If self.attr = val, go to the given subtree.\"\"\"\n",
    "        self.branches[val] = subtree\n",
    "\n",
    "    def display(self, indent=0):\n",
    "        name = self.attr_name\n",
    "        print('Test', name)\n",
    "        for (val, subtree) in self.branches.items():\n",
    "            print(' ' * 4 * indent, name, '=', val, '==>', end=' ')\n",
    "            subtree.display(indent + 1)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return 'DecisionFork({0!r}, {1!r}, {2!r})'.format(self.attr, self.attr_name, self.branches)\n",
    "\n",
    "\n",
    "class DecisionLeaf:\n",
    "    \"\"\"A leaf of a decision tree holds just a result.\"\"\"\n",
    "\n",
    "    def __init__(self, result):\n",
    "        self.result = result\n",
    "\n",
    "    def __call__(self, example):\n",
    "        return self.result\n",
    "\n",
    "    def display(self):\n",
    "        print('RESULT =', self.result)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return repr(self.result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Learning\n",
    "Decision tree learning is the construction of a decision tree from class-labeled training data. The data is expected to be a tuple in which each record of the tuple is an attribute used for classification. The decision tree is built top-down, by choosing a variable at each step that best splits the set of items. There are different metrics for measuring the \"best split\". These generally measure the homogeneity of the target variable within the subsets.\n",
    "\n",
    "#### Information Gain\n",
    "Information gain is based on the concept of entropy from information theory. Entropy is defined as:\n",
    "\n",
    "$$H(p) = -\\sum{p_i \\log_2{p_i}}$$\n",
    "\n",
    "Information Gain is difference between entropy of the parent and weighted sum of entropy of children. The feature used for splitting is the one which provides the most information gain.\n",
    "\n",
    "#### Pseudocode\n",
    "\n",
    "You can view the pseudocode by running the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudocode(\"Decision-Tree-Learning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Implementation\n",
    "Using the classes above, your next task is to implement the DecisionTreeLearner\n",
    "\n",
    "Our implementation of `DecisionTreeLearner` provided will use information gain as the metric for selecting which attribute to test for splitting. The function builds the tree top-down recursively. Based on the input it should make one of the four choices:\n",
    "\n",
    "- If the input at the current step has no training data we return the plurality value of classes of input data received in the parent step (previous level of recursion). This is conveniently available to you in the `plurality_value` method.\n",
    "- If all values in training data belong to the same class it returns a `DecisionLeaf` whose class label is the class which all the data belongs to.\n",
    "- If the data has no attributes that can be tested we return the class with the highest plurality value in the training data.\n",
    "- We choose the attribute which gives the highest amount of entropy gain (which we implement for you in the `choose_attribute` method) and return a `DecisionFork` which splits based on this attribute. Each branch recursively calls `decision_tree_learning` to construct the sub-tree.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeLearner:\n",
    "    \"\"\"[Figure 18.5]\"\"\"\n",
    "\n",
    "    def __init__(self, dataset: DataSet, size=None):\n",
    "        self.dataset = dataset\n",
    "        self.tree = self.decision_tree_learning(dataset.examples, dataset.inputs)\n",
    "\n",
    "    def decision_tree_learning(self, examples, attrs, parent_examples=()):\n",
    "        tree = None\n",
    "        #### Your Code Here ####\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        #########################\n",
    "        return tree\n",
    "\n",
    "    def plurality_value(self, examples):\n",
    "        \"\"\"\n",
    "        Return the most popular target value for this set of examples.\n",
    "        (If target is binary, this is the majority; otherwise plurality).\n",
    "        \"\"\"\n",
    "        popular = argmax_random_tie(self.dataset.values[self.dataset.target],\n",
    "                                    key=lambda v: self.count(self.dataset.target, v, examples))\n",
    "        return DecisionLeaf(popular)\n",
    "\n",
    "    def count(self, attr, val, examples):\n",
    "        \"\"\"Count the number of examples that have example[attr] = val.\"\"\"\n",
    "        return sum(e[attr] == val for e in examples)\n",
    "\n",
    "    def all_same_class(self, examples):\n",
    "        \"\"\"Are all these examples in the same target class?\"\"\"\n",
    "        class0 = examples[0][self.dataset.target]\n",
    "        return all(e[self.dataset.target] == class0 for e in examples)\n",
    "\n",
    "    def choose_attribute(self, attrs, examples):\n",
    "        \"\"\"Choose the attribute with the highest information gain.\"\"\"\n",
    "        return argmax_random_tie(attrs, key=lambda a: self.information_gain(a, examples))\n",
    "\n",
    "    def information_gain(self, attr, examples):\n",
    "        \"\"\"Return the expected reduction in entropy from splitting by attr.\"\"\"\n",
    "\n",
    "        def I(examples):\n",
    "            return information_content([self.count(self.dataset.target, v, examples)\n",
    "                                        for v in self.dataset.values[self.dataset.target]])\n",
    "\n",
    "        n = len(examples)\n",
    "        remainder = sum((len(examples_i) / n) * I(examples_i)\n",
    "                        for (v, examples_i) in self.split_by(attr, examples))\n",
    "        return I(examples) - remainder\n",
    "\n",
    "    def split_by(self, attr, examples):\n",
    "        \"\"\"Return a list of (val, examples) pairs for each val of attr.\"\"\"\n",
    "        return [(v, [e for e in examples if e[attr] == v]) for v in self.dataset.values[attr]]\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.tree(x)\n",
    "\n",
    "\n",
    "def information_content(values):\n",
    "    \"\"\"Number of bits to represent the probability distribution in values.\"\"\"\n",
    "    probabilities = normalize(remove_all(0, values))\n",
    "    return sum(-p * np.log2(p) for p in probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing our classifier\n",
    "\n",
    "We can now use our implementation to train a model using the restaurant dataset and classify a few examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## From AIMA\n",
    "from learning import cross_validation, err_ratio\n",
    "\n",
    "dt_learner = DecisionTreeLearner(restaurant)\n",
    "print(\"Classification for the first example: \"+restaurant.examples[0][restaurant.target])\n",
    "print(\"Prediction for the first example: \"+dt_learner.predict(restaurant.examples[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_learner = DecisionTreeLearner(restaurant1)\n",
    "\n",
    "mean_error = err_ratio(dt_learner, restaurant1)\n",
    "print('Trained Dataset Mean error %.2f'%mean_error)\n",
    "\n",
    "mean_error = cross_validation(DecisionTreeLearner, restaurant1)\n",
    "print('Trained Dataset Mean error %.2f'%mean_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "Now that we have looked how to learn decision trees for classification, we switch to regression algorithms. Regression algorithms assume a continuous valued output, based on the values of input features. In this practical, we focus on linear regression learners. \n",
    "A Linear Learner is a model that assumes a linear relationship between the input variables $x$ and the single output variable $y$. More specifically, that $y$ can be calculated from a linear combination of the input variables $x$. Linear learner is a quite simple model as the representation of this model is a linear equation.  \n",
    "\n",
    "The linear equation assigns one scaler factor to each input value or column, called coefficients or weights. One additional coefficient is also added, giving additional degree of freedom and is often called the intercept or the bias coefficient.   \n",
    "For example :  $y = ax_{1} + bx_{2} + c$. \n",
    "\n",
    "More generally, for multivariate linear regression, we call the current model hypothesis $h_{\\mathbf{w}}(\\vec{x})$, where $\\mathbf{w} = \\{w_{0}, w_{1}, \\dots w_{n}\\}$ are the weights, and $\\vec{x} = \\{ x_{1}, \\dots, x_{n}\\}$ is a feature vector. Thus:\n",
    "\n",
    "$$h_{\\mathbf{w}}(\\vec{x}) = w_{0} + w_{1}x_{1} + \\dots + w_{n}x_{n}$$\n",
    "\n",
    "Modern hardware acceleration allows us to perform matrix multiplication very fast, so that efficient prediction in a linear model is made using a vector of weights, which we multiply by a vector with the input features, plus a dummy feature $x_{0} = 1$ to match the intercept term:\n",
    "\n",
    "$$h_{\\mathbf{w}}(\\vec{x}) = \\mathbf{w} \\cdot \\vec{x} = \\mathbf{w}^{\\top} \\vec{x} = \\sum_{i}w_{i}x_{i}$$\n",
    "\n",
    "Our algorithm wants to generate a vector of weights $\\mathbf{w}^{*}$ that minimizes the squared error loss over examples:\n",
    "\n",
    "$$\\mathbf{w}^{*} = \\arg\\min_{\\mathbf{w}} \\sum_{j} \\left( y_{j} - \\mathbf{w} \\cdot \\vec{x}_{j} \\right)^{2}$$\n",
    "\n",
    "This algorithm first assigns some random weights to the input variables and then based on the error calculated updates the weight for each variable, using the following update rule.\n",
    "\n",
    "$$w_{i} \\gets w_{i} + \\alpha\\sum_{j}\\left( y_{j} - h_{\\mathbf{w}}(\\vec{x}_{j}) \\right) \\times x_{j,i}$$\n",
    "\n",
    "Finally, the prediction is made with the updated weights.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils4e import random_weights\n",
    "\n",
    "class LinearRegressionLearner:\n",
    "    \"\"\"\n",
    "    [Section 18.6.3]\n",
    "    Multivariate Linear ression.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset: DataSet, learning_rate=0.01, epochs=100):\n",
    "        \n",
    "        idx_i = dataset.inputs\n",
    "        idx_t = dataset.target\n",
    "        examples = dataset.examples\n",
    "        num_examples = len(examples)\n",
    "\n",
    "        # X transpose\n",
    "        X_col = list(zip(*dataset.examples))  # vertical columns of X\n",
    "        X_col = [X_col[i] for i in idx_i]\n",
    "\n",
    "        # add dummy\n",
    "        ones = [1 for _ in range(len(examples))]\n",
    "        X_col = [ones] + X_col\n",
    "\n",
    "        # initialize random weights\n",
    "        num_weights = len(idx_i) + 1\n",
    "        w = random_weights(min_value=-0.5, max_value=0.5, num_weights=num_weights)\n",
    "\n",
    "        err = [0]\n",
    "\n",
    "        iter = tqdm(range(epochs),postfix=\"Error: %f\"%(np.mean(err)))\n",
    "        for epoch in iter:\n",
    "            err = []\n",
    "            # pass over all examples\n",
    "            for example in examples:\n",
    "                x = [1] + [example[i] for i in idx_i] ## We add 1 to the example array because we assume the first weight is the bias coefficient.\n",
    "                # Compute the prediction with the current weights\n",
    "                #### Your code here (1 line) ####\n",
    "                y = None\n",
    "                ##################################\n",
    "                t = example[idx_t]\n",
    "                err.append(t - y)\n",
    "\n",
    "            # update weights\n",
    "            for i in range(len(w)):\n",
    "                # Compute the overall loss and update each parameter\n",
    "                #### Your code here (2 lines) ####\n",
    "                loss = None\n",
    "                w[i] = None\n",
    "                ##################################\n",
    "            iter.set_postfix_str(\"Error: %f\"%(np.mean(err)))\n",
    "        self.w = w\n",
    "        # print('Finished training, final error is %f'%np.mean(err))\n",
    "\n",
    "    def predict(self, example):\n",
    "        x = [1] + example\n",
    "        return np.dot(self.w, x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "We need a numeric dataset for linear regression. The dataset below is one such dataset, which we plot for you to inspect your model later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# produce vector inline graphics\n",
    "%matplotlib inline\n",
    "# from IPython.display import set_matplotlib_formats\n",
    "# set_matplotlib_formats('pdf', 'svg')\n",
    "import matplotlib_inline.backend_inline\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats('svg')\n",
    "\n",
    "from matplotlib import pyplot\n",
    "pyplot.rcParams['text.usetex'] = False\n",
    "\n",
    "\n",
    "simple_data = DataSet(name='data1', target='Y', examples=None,\n",
    "                   attr_names='X Y')\n",
    "\n",
    "fig = pyplot.figure()  # open a new figure\n",
    "data = np.array(simple_data.examples)\n",
    "\n",
    "pyplot.plot(data[:,0], data[:,-1], 'ro', ms=10, mec='k')\n",
    "pyplot.ylabel('Y')\n",
    "pyplot.xlabel('X')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the dataset\n",
    "\n",
    "We can now use the linear regression learner we implement to find a model to predict points in this space. For your convenience, we also plot the curve your model induces in this space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lr_learner = LinearRegressionLearner(simple_data)\n",
    "\n",
    "# print(lr_learner.w)\n",
    "\n",
    "fig = pyplot.figure()  # open a new figure\n",
    "data = np.array(simple_data.examples)\n",
    "\n",
    "pyplot.plot(data[:,0], data[:,-1], 'ro', ms=10, mec='k')\n",
    "pyplot.ylabel('Y')\n",
    "pyplot.xlabel('X')\n",
    "x_s = np.arange(0,25)\n",
    "y_s = [lr_learner.predict([x]) for x in x_s]\n",
    "pyplot.plot(x_s,y_s , color=\"green\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "a04fbcb04a11c3a691703ca23ae4a78f9f717b538476169d44c620972eb30c1e"
   }
  },
  "colab": {
			"provenance": [],
			"generative_ai_disabled": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
